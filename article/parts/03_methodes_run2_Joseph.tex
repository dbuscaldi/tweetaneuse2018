Notre deuxième système utilise des réseaux de neurones récurrents pour implanter un classificateur, plus spécifiquement les LSTM~\cite{hochreiter1997long} qui sont largement utilisés en traitement automatiques des langues.
La classification se fait en trois temps~:
\begin{enumerate}
\item Le texte est séparé aux espaces.
  Chaque segment est traité comme une séquence d'octets lue de gauche à droite et de droite à gauche par deux réseaux récurrents \emph{niveau caractère}.
  Les vecteurs résultats des lectures sont additionnés et servent de représentation du segment, dite compositionnelle.
  Pour une séquence de caractères $s = c_{1}\ldots c_{m}$, on calcule pour chaque position $h_{i} = LSTM_{o}(h_{i-1},e(c_{i}))$ et $h'_{i} = LSTM_{o'}(h'_{i+1},e(c_{i}))$, où $e$ est la fonction de plongement des caractères vers les vecteurs denses, et $LSTM$ est un raccourci pour une fonction  implantant la cellule récurrente des {LSTM}.
  La représentation compositonnelle du segment est $c(s) = h_{m} + h'_{1}$

\item La séquence de segments est lue à nouveau de gauche à droite et de droite à gauche par de nouveaux réseaux récurrents \emph{niveau mot} qui prennent en entrée pour chaque segment la représentation compositionnelle venant de l'étape précédente à laquelle on ajoute une représentation vectorielle du segment si celui-ci était présent  plus de 10 fois dans le corpus d'entraînement.
  Pour une séquence de segments $p = s_{1} \ldots s_{n}$, on calcule
  $l_{i} = LSTM_{m}(l_{i-1},c(s_{i}) + e(s_{i}))$, $l'_{i} =
  LSTM_{m'}(l_{i+1},c(s_{i}) + e(s_{i}))$, où $c$ est la représentation compositionnelle donnée ci-dessus et $e$ la fonction de plongement que l'on étend aux segments vus dans l'ensemble d'entraînement.
  Les états finaux obtenues après lecture dans les deux directions sont sommés et servent de représentation de la phrase d'entrée, $r(p) = l_{n} + l'_{1}$.

\item La représentation obtenue sert d'entrée à un perceptron multi-niveau qui effectue la classification finale, aussi bien pour la tâche 1 que pour la tâche 2:  $o(p) = \sigma(O \times \max(0, (W \times r(p) + b)))$ où $\sigma$ est l'opérateur \emph{softmax}, $W$, $O$ des matrices et $b$ un vecteur. On interprète la sortie comme une distribution de probabilité sur les classes de \textit{tweets}.
\end{enumerate}

Cette interprétation probabiliste nous permet de réduire l'apprentissage des paramètres du système (\emph{ie.} les plongements de caractères et de segments fréquents, $O,W,b$, ainsi que les paramètres des 4 cellules LSTM) à la maximisation de la vraisemblance du corpus d'entraînement.
On utilise l'algorithme AMSgrad~\cite{reddi2018on} pour  calculer la taille du pas lors de la descente de gradient.
Pour éviter le sur-apprentissage, nous  procédons aux deux ajustements suivants.
\begin{itemize}
\item On écarte aléatoirement du corpus d'entraînement 10\% des phrases qui sont utilisées comme ensemble de validation, ce qui permet de décider quand les paramètres sont toujours utiles sur des données inconnues.
\item on utilise la technique du \emph{dropout}~\cite{srivastava2014dropout}, sur tous les vecteurs à chaque étage du réseau --- sauf la couche finale bien sûr.
  Durant l'apprentissage les neurones sont aléatoirement remis à zéro avec une probabilité de 0,5.

\end{itemize}

Les plongements de caractères sont de taille 16, ceux des segments de taille 32, la couche d'entrée du perceptron est de taille 64 et la couche cachée de taille 32.
La couche de sortie est de taille 2 pour la tâche 1, et 4 pour la tâche 2.
Nous utilisons la bibliothèque \textsc{Dynet}\footnote{\url{https://github.com/clab/dynet}} avec les paramètres par défaut.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../tweetaneuse"
%%% End:
