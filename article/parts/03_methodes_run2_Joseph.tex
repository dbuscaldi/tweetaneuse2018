Notre deuxième système utilise des réseaux de neurones récurrents pour implanter un classificateur, plus spécifiquement les LSTM \textbf{REF} qui sont largement utilisés en traitement automatiques des langues.
La classification se fait en trois temps~:
\begin{enumerate}
\item Le texte est séparé aux espaces.
  Chaque segment est traité comme une séquence d'octets lue de gauche à droite et de droite à gauche par deux réseaux récurrents \emph{niveau caractère}.
  Les vecteurs résultats des lectures sont additionnés et servent de
  représentation du segment, dite compositionnelle. Pour une séquence
  de caractères $s = c_{1}\ldots c_{m}$, on calcule pour chaque position
  $h_{i} = LSTM_{o}(h_{i-1},c_{i})$ et $h'_{i} =
  LSTM_{o'}(h'_{i+1},c_{i})$. La représentation compositonnelle du
  segment est $c(s) = h_{m} + h'_{1}$

\item La séquence de segments est lue à nouveau de gauche à droite et
  de droite à gauche par de nouveaux réseaux récurrents \emph{niveau
    mot} qui prennent en entrée pour chaque segment la représentation
  compositionnelle venant de l'étape précédente à laquelle on ajoute
  une représentation vectorielle du segment si celui-ci était présents
  plus de 10 fois dans le corpus d'entraînement.
  Pour une séquence de segments $p = s_{1} \ldots s_{n}$, on calcule
  $l_{i} = LSTM_{m}(l_{i-1},c(s_{i}) + e(s_{i}))$, $l'_{i} =
  LSTM_{m'}(l_{i+1},c(s_{i}) + e(s_{i}))$.

   Les états finaux obtenues après lecture dans les deux directions
  sont sommés et servent de représentation de la phrase d'entrée,
  $r(p) = l_{n} + l'_{1}$.

\item La représentation obtenue sert d'entrée à un perceptron multi-niveau qui
  effectue la classification finale, aussi bien pour la tâche 1 que
  pour la tâche 2:  $o(p) = \sigma(O \times \max(0, (W \times r(p) +
  b)))$ où $\sigma$ est l'opérateur \emph{softmax}.
\end{enumerate}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../tweetaneuse"
%%% End:
