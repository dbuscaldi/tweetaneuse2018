Notre deuxième système utilise des réseaux de neurones récurrents pour implanter un classificateur, plus spécifiquement les LSTM~\cite{hochreiter1997long} qui sont largement utilisés en traitement automatiques des langues.
La classification se fait en trois temps~:
\begin{enumerate}
\item Le texte est séparé aux espaces.
  Chaque segment est traité comme une séquence d'octets lue de gauche à droite et de droite à gauche par deux réseaux récurrents \emph{niveau caractère}.
  Les vecteurs résultats des lectures sont additionnés et servent de représentation du segment, dite compositionnelle.
  Pour une séquence de caractères $s = c_{1}\ldots c_{m}$, on calcule pour chaque position $h_{i} = LSTM_{o}(h_{i-1},e(c_{i}))$ et $h'_{i} = LSTM_{o'}(h'_{i+1},e(c_{i}))$, où $e$ est la fonction de plongement des caractères vers les vecteurs denses, et $LSTM$ est un raccourci pour une fonction  implantant la cellule récurrente des LSTM.
  La représentation compositonnelle du segment est $c(s) = h_{m} + h'_{1}$

\item La séquence de segments est lue à nouveau de gauche à droite et de droite à gauche par de nouveaux réseaux récurrents \emph{niveau mot} qui prennent en entrée pour chaque segment la représentation compositionnelle venant de l'étape précédente à laquelle on ajoute une représentation vectorielle du segment si celui-ci était présent  plus de 10 fois dans le corpus d'entraînement.
  Pour une séquence de segments $p = s_{1} \ldots s_{n}$, on calcule
  $l_{i} = LSTM_{m}(l_{i-1},c(s_{i}) + e(s_{i}))$, $l'_{i} =
  LSTM_{m'}(l_{i+1},c(s_{i}) + e(s_{i}))$, où $c$ est la représentation compositionnelle donnée ci-dessus et $e$ une fonction de plongement du segment.

  Les états finaux obtenues après lecture dans les deux directions sont sommés et servent de représentation de la phrase d'entrée, $r(p) = l_{n} + l'_{1}$.

\item La représentation obtenue sert d'entrée à un perceptron multi-niveau qui effectue la classification finale, aussi bien pour la tâche 1 que pour la tâche 2:  $o(p) = \sigma(O \times \max(0, (W \times r(p) + b)))$ où $\sigma$ est l'opérateur \emph{softmax}, $W$, $O$ des matrices et $b$ un vecteur.
\end{enumerate}

L'apprentissage du système se fait en maximisant la vraisemblance du corpus d'entraînement.
On utilise l'algorithme AMSgrad pour  calculer la taille du pas lors de la descente de gradient.
On écarte aléatoirement du corpus d'entraînement 10\% des phrases qui sont utilisées comme ensemble de validation.
Les plongements de caractères sont de taille 16, ceux des segments de taille 32, la couche d'entrée du perceptron est de taille 64 et la couche cachée de taille 32.
La couche de sortie est de taille 2 pour la tâche 1, et 4 pour la tâche 2.
Pour éviter le sur-apprentissage nous utilisons la technique du \emph{dropout} \cite{srivastava2014dropout}, sur tous les vecteurs à chaque étage du réseau (sauf la couche finale bien sûr)~: durant l'apprentissage les neurones sont aléatoirement mis à zéro avec une probabilité de 0,5.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../tweetaneuse"
%%% End:
